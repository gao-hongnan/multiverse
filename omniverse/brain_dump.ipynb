{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Playbook: Sampling Methodologies in Large Language Models](#toc1_)    \n",
    "  - [Sampling Methodologies (Deep Learning: Foundations and Concepts)](#toc1_1_)    \n",
    "  - [Multinomial](#toc1_2_)    \n",
    "  - [Categorical Distribution Overview](#toc1_3_)    \n",
    "  - [Multinomial Distribution](#toc1_4_)    \n",
    "  - [Formalizing Multinomial Sampling](#toc1_5_)    \n",
    "  - [Greedy vs Probabilistic sampling](#toc1_6_)    \n",
    "- [How to Derive Sigmoid and Softmax Functions from Exponential Family in Machine Learning Context](#toc2_)    \n",
    "  - [Bernoulli Distribution as an Exponential Family Member](#toc2_1_)    \n",
    "  - [Expressing Bernoulli in Exponential Family Form](#toc2_2_)    \n",
    "  - [Derivation of the Sigmoid Function](#toc2_3_)    \n",
    "- [Softmax -> CrossEntropyLoss -> KL Divergence](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Playbook: Sampling Methodologies in Large Language Models](#toc0_)\n",
    "\n",
    "Main thing is to answer the question: why does temperature in llm enable the\n",
    "model to be more random when it is high and more deterministic when it is low?\n",
    "worth noting that if greedy sampling then it is deterministic, if sampling from\n",
    "multinomial then it is random - and dependent on softmax. but if softmax\n",
    "preserves order then how does it become more random? Precisely why i said greedy\n",
    "sampling is deterministic since the order is preserved but multinomial sampling\n",
    "is random but the order is also preserved - the key lies in the sharpen/dampen\n",
    "effect of the softmax distribution. If sharp, means dominated by 1 value\n",
    "usually, 0.99, so samplign from that converges to greedy sampling -\n",
    "deterministic. If dampened, means more uniform, so sampling from that converges\n",
    "to more diverse sampling. But is if converge to uniform no right.\n",
    "\n",
    "-   softmax sharpens/dampens distribution\n",
    "-   multinomial enables randomness\n",
    "-   greedy sampling enables deterministic\n",
    "-   and multinomial with T=0 converges to greedy = deterministic sampling```\n",
    "\n",
    "And SINCE the softmax function is not invariant under scaling, we can introduce\n",
    "a temperature parameter $T$ to control the entropy of the output distribution.\n",
    "BECAUSE TEMPERATURE IS EFFECTIVELY SCALING! The temperature is a way to control\n",
    "the entropy of a distribution, while preserving the relative ranks of each\n",
    "event.\n",
    "\n",
    "\n",
    "## <a id='toc1_1_'></a>[Sampling Methodologies (Deep Learning: Foundations and Concepts)](#toc0_)\n",
    "\n",
    "We have seen that the output of a decoder transformer is a probability\n",
    "distribution over values for the next token in the sequence, from which a\n",
    "particular value for that token must be chosen to extend the sequence. There are\n",
    "several options for selecting the value of the token based on the computed\n",
    "probabilities (Holtzman et al., 2019). One obvious approach, called greedy\n",
    "search, is simply to select the token with the highest probability. This has the\n",
    "effect of making the model deterministic, in that a given input sequence always\n",
    "generates the same output sequence. Note that simply choosing the highest\n",
    "probability token at each stage is not the same as selecting the highest\n",
    "probability sequence of tokens. To find the most probable sequence, we would\n",
    "need to maximize the joint distribution over all tokens, which is given by\n",
    "\n",
    "$$\n",
    "p\\left(\\mathbf{y}_1, \\ldots, \\mathbf{y}_N\\right)=\\prod_{n=1}^N p\\left(\\mathbf{y}_n \\mid \\mathbf{y}_1, \\ldots, \\mathbf{y}_{n-1}\\right)\n",
    "$$\n",
    "\n",
    "If there are $N$ steps in the sequence and the number of token values in the\n",
    "dictionary is $K$ then the total number of sequences is\n",
    "$\\mathcal{O}\\left(K^N\\right)$, which grows exponentially with the length of the\n",
    "sequence, and hence finding the single most probable sequence is infeasible. By\n",
    "comparison, greedy search has cost $\\mathcal{O}(K N)$, which is linear in the\n",
    "sequence length.\n",
    "\n",
    "One technique that has the potential to generate higher probability sequences\n",
    "than greedy search is called beam search. Instead of choosing the single most\n",
    "probable token value at each step, we maintain a set of $B$ hypotheses, where\n",
    "$B$ is called the beam width, each consisting of a sequence of token values up\n",
    "to step $n$. We then feed all these sequences through the network, and for each\n",
    "sequence we find the $B$ most probable token values, thereby creating $B^2$\n",
    "possible hypotheses for the extended sequence. This list is then pruned by\n",
    "selecting the most probable $B$ hypotheses according to the total probability of\n",
    "the extended sequence. Thus, the beam search algorithm maintains $B$ alternative\n",
    "sequences and keeps track of their probabilities, finally selecting the most\n",
    "probable sequence amongst those considered. Because the probability of a\n",
    "sequence is obtained by multiplying the probabilities at each step of the\n",
    "sequence and since these probability are always less than or equal to one, a\n",
    "long sequence will generally have a lower probability than a short one, biasing\n",
    "the results towards short sequences. For this reason the sequence probabilities\n",
    "are generally normalized by the corresponding lengths of the sequence before\n",
    "making comparisons. Beam search has cost $\\mathcal{O}(B K N)$, which is again\n",
    "linear in the sequence length. However, the cost of generating a sequence is\n",
    "increased by a factor of $B$, and so for very large language models, where the\n",
    "cost of inference can become significant, this makes beam search much less\n",
    "attractive.\n",
    "\n",
    "One problem with approaches such as greedy search and beam search is that they\n",
    "limit the diversity of potential outputs and can even cause the generation\n",
    "process to become stuck in a loop, where the same sub-sequence of words is\n",
    "repeated over and over. As can be seen in Figure 12.17, human-generated text may\n",
    "have lower probability and hence be more surprising with respect to a given\n",
    "model than automatically generated text.\n",
    "\n",
    "Instead of trying to find a sequence with the highest probability, we can\n",
    "instead generate successive tokens simply by sampling from the softmax\n",
    "distribution at each step. However, this can lead to sequences that are\n",
    "nonsensical. This arises from the typically very large size of the token\n",
    "dictionary, in which there is a long tail of many token states each of which has\n",
    "a very small probability but which in aggregate account for a significant\n",
    "fraction of the total probability mass. This leads to the problem in which there\n",
    "is a significant chance that the system will make a bad choice for the next\n",
    "token.\n",
    "\n",
    "As a balance between these extremes, we can consider only the states having the\n",
    "top $K$ probabilities, for some choice of $K$, and then sample from these\n",
    "according to their renormalized probabilities. A variant of this approach,\n",
    "called top- $p$ sampling or nucleus sampling, calculates the cumulative\n",
    "probability of the top outputs until a threshold is reached and then samples\n",
    "from this restricted set of token states.\n",
    "\n",
    "A 'softer' version of top- $K$ sampling is to introduce a parameter $T$ called\n",
    "temperature into the definition of the softmax function (Hinton, Vinyals, and\n",
    "Dean, 2015) so that\n",
    "\n",
    "$$\n",
    "y_i=\\frac{\\exp \\left(a_i / T\\right)}{\\sum_j \\exp \\left(a_j / T\\right)}\n",
    "$$\n",
    "\n",
    "and then sample the next token from this modified distribution. When $T=0$, the\n",
    "probability mass is concentrated on the most probable state, with all other\n",
    "states having zero probability, and hence this becomes greedy selection. For\n",
    "$T=1$, we recover the unmodified softmax distribution, and as\n",
    "$T \\rightarrow \\infty$, the distribution becomes uniform across all states. By\n",
    "choosing a value in the range $0<T<1$, the probability is concentrated towards\n",
    "the higher values.\n",
    "\n",
    "One challenge with sequence generation is that during the learning phase, the\n",
    "model is trained on a human-generated input sequence, whereas when it is running\n",
    "generatively, the input sequence is itself generated from the model. This means\n",
    "that the model can drift away from the distribution of sequences seen during\n",
    "training.\n",
    "\n",
    "\n",
    "## <a id='toc1_2_'></a>[Multinomial](#toc0_)\n",
    "\n",
    "In multinomial (or probabilistic) sampling, the model samples from the entire\n",
    "probability distribution obtained after applying softmax:\n",
    "\n",
    "At higher temperatures, the probability distribution becomes more uniform,\n",
    "increasing the likelihood of sampling less probable tokens, thereby introducing\n",
    "more randomness or diversity into the selection process. At lower temperatures,\n",
    "the distribution becomes sharper, concentrating most of the probability mass on\n",
    "a few high-probability tokens. This makes the selection less random and more\n",
    "predictable, closely aligning with the greedy selection outcome but still\n",
    "allowing for some variability.\n",
    "\n",
    "KEY is to understand multinomial in the sampling.\n",
    "\n",
    "## <a id='toc1_3_'></a>[Categorical Distribution Overview](#toc0_)\n",
    "\n",
    "The categorical distribution is a probability distribution that describes the\n",
    "result of a random event that can take on one of $K$ possible outcomes, with\n",
    "each outcome having its own probability. It is the generalization of the\n",
    "Bernoulli distribution for variables with more than two states.\n",
    "\n",
    "Let $Y$ be a discrete random variable that can take on $K$ different states. We\n",
    "say $Y$ follows a categorical distribution with parameters\n",
    "$\\boldsymbol{\\pi} = (\\pi_1, \\pi_2, ..., \\pi_K)$ if the probability of $Y$ taking\n",
    "on the value $k$ is given by:\n",
    "\n",
    "$$\\mathbb{P}(Y=k)=\\pi_k \\quad \\text{for } k=1,2, \\ldots, K$$\n",
    "\n",
    "Here, $\\pi*k$ represents the probability of the event that $Y=k$, with the\n",
    "constraint that $\\sum*{k=1}^K \\pi_k = 1$.\n",
    "\n",
    "The probability mass function (PMF) of $Y$ is then compactly defined as:\n",
    "\n",
    "$$\\mathbb{P}(Y=k)=\\prod\\_{k=1}^K \\pi_k^{I\\{Y=k\\}}$$\n",
    "\n",
    "where $I\\{Y=k\\}$ is the indicator function, equal to 1 if $Y=k$ and 0 otherwise.\n",
    "This PMF formulation concisely represents the categorical distribution,\n",
    "highlighting that each $\\pi_k$ contributes to the probability mass function only\n",
    "when $Y=k$.\n",
    "\n",
    "## <a id='toc1_4_'></a>[Multinomial Distribution](#toc0_)\n",
    "\n",
    "The multinomial distribution generalizes the categorical distribution to\n",
    "multiple independent trials, each of which results in a categorically\n",
    "distributed outcome.\n",
    "\n",
    "Consider a random experiment that results in one of $K$ possible outcomes, with\n",
    "each outcome having a fixed probability $\\pi*k$, where $\\sum*{k=1}^K\n",
    "\\pi_k = 1$.\n",
    "If we repeat the experiment $n$ times independently, the vector\n",
    "$\\mathbf{Y} = (Y_1, Y_2, ..., Y_K)$ describing the number of times each outcome\n",
    "occurs follows a multinomial distribution with parameters $n$ and\n",
    "$\\boldsymbol{\\pi}$.\n",
    "\n",
    "The probability of observing a specific outcome vector\n",
    "$\\mathbf{y} = (y_1, y_2, ..., y_K)$, where $\\sum*{k=1}^K y_k = n$, is given by:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{Y}=\\mathbf{y}; n, \\boldsymbol{\\pi}) =\n",
    "\\frac{n!}{y_1!y_2!\\cdots y_K!} \\prod*{k=1}^K \\pi_k^{y_k}\n",
    "$$\n",
    "\n",
    "In this formulation:\n",
    "\n",
    "-   $n$ is the total number of trials.\n",
    "-   $y_k$ is the number of times outcome $k$ occurs in $n$ trials.\n",
    "-   $\\pi_k$ is the probability of outcome $k$ occurring in a single trial.\n",
    "\n",
    "---\n",
    "\n",
    "The multinomial distribution is a generalization of the binomial distribution.\n",
    "It models the probabilities of observing counts among multiple categories and is\n",
    "parametrized by probabilities $\\pi_1, \\pi_2, \\ldots, \\pi_n$ corresponding to $n$\n",
    "outcomes. These probabilities must satisfy two conditions:\n",
    "\n",
    "1. $0 \\leq \\pi_i \\leq 1$ for all $i$,\n",
    "2. $\\sum\\_{i=1}^{n} \\pi_i = 1$.\n",
    "\n",
    "Given a single trial, the probability of outcome $i$ occurring is $\\pi_i$. When\n",
    "sampling from a multinomial distribution, each sample (or draw) is independent,\n",
    "and the probability of observing a specific outcome follows the distribution\n",
    "defined by $\\pi$.\n",
    "\n",
    "## <a id='toc1_5_'></a>[Formalizing Multinomial Sampling](#toc0_)\n",
    "\n",
    "In the context of generative models, after computing the softmax distribution\n",
    "over the logits (or scores) $z_i$ for each token $i$ in the vocabulary, the\n",
    "softmax function at temperature $T$ is applied to obtain probabilities:\n",
    "\n",
    "$$p*i = \\frac{\\exp(z_i / T)}{\\sum_{j=1}^{n} \\exp(z_j / T)}$$\n",
    "\n",
    "Here, $p_i$ represents the probability of selecting token $i$ as the next token\n",
    "in the sequence, forming a probability distribution\n",
    "$\\pi = [p_1, p_2, \\ldots, p_n]$ over the vocabulary.\n",
    "\n",
    "Given $\\pi$, multinomial sampling draws a sample $s$ where $P(s=i) = p_i$. This\n",
    "process can be repeated to generate sequences of tokens.\n",
    "\n",
    "\n",
    "## <a id='toc1_6_'></a>[Greedy vs Probabilistic sampling](#toc0_)\n",
    "\n",
    "If your model employs a greedy strategy for selecting tokens (e.g., always\n",
    "choosing the token with the highest probability), then adjusting the temperature\n",
    "won't change the selected token. This approach is common in tasks where\n",
    "precision is critical, and the aim is to reduce randomness to a minimum, such as\n",
    "in certain classification tasks or when generating text where maximum coherence\n",
    "is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[How to Derive Sigmoid and Softmax Functions from Exponential Family in Machine Learning Context](#toc0_)\n",
    "\n",
    "REFERENCE:\n",
    "\n",
    "-   6.2.2.2, 6.2.2.3 of The Deep Learning Book by Goodfellow et al.\n",
    "-   3.4. of Deep Learning: Foundations and Concepts by Bishop et al.\n",
    "\n",
    "SAMPLE CONTENT BELOW (TO BE REFINED): We can derive via exponential family.\n",
    "\n",
    "## <a id='toc2_1_'></a>[Bernoulli Distribution as an Exponential Family Member](#toc0_)\n",
    "\n",
    "The Bernoulli distribution can be expressed in the exponential family form as\n",
    "follows:\n",
    "\n",
    "$$\n",
    "p(y | \\eta) = b(y) \\exp(\\eta^T y - A(\\eta))\n",
    "$$\n",
    "\n",
    "For the Bernoulli distribution:\n",
    "\n",
    "-   $y$ is the binary outcome (0 or 1).\n",
    "-   $\\eta$ (or $\\theta$ in some formulations) is the natural parameter of the\n",
    "    distribution.\n",
    "-   The base measure $b(y) = 1$, since it doesn't affect the form of the\n",
    "    Bernoulli distribution.\n",
    "-   The sufficient statistic $y$ is the identity function of the outcome.\n",
    "\n",
    "To match the Bernoulli distribution to the exponential family form, we recognize\n",
    "that $p(y | \\eta)$ for $y \\in \\{0,1\\}$ is given by\n",
    "$p(y = 1 | \\eta) =\n",
    "\\sigma(\\eta)$ and $p(y = 0 | \\eta) = 1 - \\sigma(\\eta)$, where\n",
    "$\\sigma(\\eta)$ is the sigmoid function. The Bernoulli distribution can be\n",
    "written as:\n",
    "\n",
    "$$\n",
    "p(y | \\eta) = \\sigma(\\eta)^y (1 - \\sigma(\\eta))^{1-y}\n",
    "$$\n",
    "\n",
    "## <a id='toc2_2_'></a>[Expressing Bernoulli in Exponential Family Form](#toc0_)\n",
    "\n",
    "Let's express the Bernoulli distribution in the form that highlights the\n",
    "exponential family structure. To do this, note that the probability of success\n",
    "$p = \\sigma(\\eta)$ where $\\sigma(\\eta)$ is the sigmoid function. By definition:\n",
    "\n",
    "$$\n",
    "\\sigma(\\eta) = \\frac{1}{1 + e^{-\\eta}}\n",
    "$$\n",
    "\n",
    "So, we have:\n",
    "\n",
    "$$\n",
    "p(y | \\eta) = \\frac{e^{\\eta y}}{1 + e^{\\eta}}\n",
    "$$\n",
    "\n",
    "where the natural parameter $\\eta = \\log\\left(\\frac{p}{1-p}\\right)$, and\n",
    "$A(\\eta) = -\\log(1 - p) = \\log(1 + e^{\\eta})$.\n",
    "\n",
    "## <a id='toc2_3_'></a>[Derivation of the Sigmoid Function](#toc0_)\n",
    "\n",
    "The sigmoid function is derived as the transformation of the natural parameter\n",
    "$\\eta$ back to the probability $p$. From the natural parameter, we have\n",
    "$\\eta = \\log\\left(\\frac{p}{1-p}\\right)$, solving for $p$ gives us the sigmoid\n",
    "function:\n",
    "\n",
    "$$\n",
    "\\eta = \\log\\left(\\frac{p}{1-p}\\right) \\Rightarrow e^{\\eta} = \\frac{p}{1-p} \\Rightarrow p = \\frac{e^{\\eta}}{1 + e^{\\eta}}\n",
    "$$\n",
    "\n",
    "By substituting $\\eta$ back with a linear combination of features (e.g.,\n",
    "$z =\n",
    "\\boldsymbol{w}^\\top \\boldsymbol{x} + b$ in machine learning), we obtain the\n",
    "sigmoid function used for binary classification:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "In this manner, the sigmoid function is derived as a special case of applying\n",
    "the exponential family formulation to the Bernoulli distribution, with the\n",
    "natural parameter $\\eta$ serving as the link between the linear predictor and\n",
    "the probabilities of the outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Softmax -> CrossEntropyLoss -> KL Divergence](#toc0_)\n",
    "\n",
    "Building on my moderately comprehensive softmax docs, we can talk about ce loss\n",
    "and kl divergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary and Sufficient\n",
    "\n",
    "1. Necessary Condition:\n",
    "\n",
    "    - If condition A is necessary for condition B, it means that B cannot be\n",
    "      true unless A is also true.\n",
    "    - In other words, if B is true, then A must be true as well.\n",
    "    - Example: \"Being a mammal is necessary for being a cat.\" If an animal is a\n",
    "      cat, it must be a mammal.\n",
    "\n",
    "2. Sufficient Condition:\n",
    "\n",
    "    - If condition A is sufficient for condition B, it means that if A is true,\n",
    "      then B must be true as well.\n",
    "    - However, B can still be true even if A is not true.\n",
    "    - Example: \"Being a cat is sufficient for being a mammal.\" If an animal is a\n",
    "      cat, it is definitely a mammal. However, not all mammals are cats.\n",
    "\n",
    "3. Necessary and Sufficient Condition:\n",
    "\n",
    "    - If condition A is both necessary and sufficient for condition B, it means\n",
    "      that A and B are equivalent.\n",
    "    - In other words, B is true if and only if A is true.\n",
    "    - Example: \"Having four sides of equal length and four right angles is\n",
    "      necessary and sufficient for being a square.\" A shape is a square if and\n",
    "      only if it has four equal sides and four right angles.\n",
    "\n",
    "4. Necessary but Not Sufficient Condition:\n",
    "    - If condition A is necessary but not sufficient for condition B, it means\n",
    "      that A must be true for B to be true, but A being true does not guarantee\n",
    "      that B is true.\n",
    "    - In other words, B cannot be true without A, but the presence of A does not\n",
    "      automatically make B true.\n",
    "    - Example: \"Being a mammal is necessary but not sufficient for being a cat.\"\n",
    "      All cats are mammals, but not all mammals are cats. Being a mammal is a\n",
    "      requirement for being a cat, but it doesn't automatically make an animal a\n",
    "      cat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
